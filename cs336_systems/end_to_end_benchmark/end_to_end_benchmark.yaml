model_config:
  vocab_size: 32000
  context_length: 2048
  d_model: 768
  num_layers: 12
  num_heads: 12
  d_ff: 3072
  rope_theta: 10000.0

batch_size: 8
seq_length: 512
warmup_steps: 10
profile_steps: 50
include_backward: true